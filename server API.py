import httpx
import logging
from logging import StreamHandler
from colorlog import ColoredFormatter
from flask import Flask, request, jsonify, Response
import asyncio
import configparser
import traceback
from sentence_transformers import SentenceTransformer, util
import re

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–ª–∞–≥–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–µ—Ä–∞
server_ready = False

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
LOG_LEVEL = logging.INFO
LOGFORMAT = "%(log_color)s%(asctime)s [%(levelname)s] %(message)s%(reset)s"

# –¶–≤–µ—Ç–æ–≤–∞—è —Å—Ö–µ–º–∞
COLOR_SCHEME = {
    'DEBUG':    'cyan',
    'INFO':     'green',
    'WARNING':  'yellow',
    'ERROR':    'red',
    'CRITICAL': 'bold_red',
}

formatter = ColoredFormatter(
    LOGFORMAT,
    datefmt='%Y-%m-%d %H:%M:%S',
    reset=True,
    log_colors=COLOR_SCHEME
)

handler = StreamHandler()
handler.setFormatter(formatter)

logging.root.setLevel(LOG_LEVEL)
logging.root.addHandler(handler)
logging.getLogger("httpx").setLevel(logging.WARNING)

# Flask app initialization
app = Flask(__name__)

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç–∞ /status
@app.route('/status', methods=['GET'])
def status():
    if server_ready:
        return jsonify({'status': 'ready'}), 200
    else:
        return jsonify({'status': 'not_ready'}), 503

# –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è
@app.route('/api/message', methods=['POST'])
async def handle_message():
    if not server_ready:
        return jsonify({'error': 'Server is not ready'}), 503

    logging.info("üöÄ –ü–æ–ª—É—á–µ–Ω –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.")
    data = request.get_json()
    if not data:
        logging.error("‚ùå –ù–µ—Ç –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.")
        return jsonify({'error': 'No input data provided'}), 400

    user_message = data.get('message')
    if not user_message:
        logging.error("‚ùå –ù–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
        return jsonify({'error': 'No message provided'}), 400

    logging.info(f"üí¨ –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_message}")
    is_complex, subqueries = await analyze_and_decompose_query_with_llm(user_message)

    if is_complex:
        logging.info(f"üõ†Ô∏è –ù–∞–π–¥–µ–Ω—ã –ø–æ–¥–∑–∞–ø—Ä–æ—Å—ã ({len(subqueries)}):")
        for subquery in subqueries:
            logging.info(f"  üî∏ {subquery}")
        tasks = [process_subquery(subquery) for subquery in subqueries]
        try:
            assistant_responses = await asyncio.gather(*tasks)
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–æ–¥–∑–∞–ø—Ä–æ—Å–æ–≤: {e}")
            return jsonify({'error': str(e)}), 500

        final_response = await aggregate_responses_with_llm(assistant_responses)
        if "Error" in final_response:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º –æ—Ç–≤–µ—Ç–µ: {final_response}")
            return jsonify({'error': final_response}), 500
    else:
        if "Error" in subqueries[0]:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø–æ–¥–∑–∞–ø—Ä–æ—Å–∞—Ö: {subqueries[0]}")
            return jsonify({'error': subqueries[0]}), 500

        messages = [
            {"role": "system", "content": default_system_content},
            {"role": "user", "content": user_message}
        ]
        try:
            response = await openai_post_request(
                messages, decomposition_model_name, decomposition_max_tokens, decomposition_temperature
            )
            final_response = response['choices'][0]['message']['content'].strip()
            logging.info(f"üéØ –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {final_response}")
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Å—Ç–æ–º –∑–∞–ø—Ä–æ—Å–µ: {e}")
            return jsonify({'error': str(e)}), 500

    return jsonify({'response': final_response})

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞
async def analyze_and_decompose_query_with_llm(user_message):
    logging.info("üîé –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞.")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã –∑–∞–ø—Ä–æ—Å–∞
    word_count = len(user_message.strip().split())
    if word_count <= 10:
        logging.info("‚úÖ –ó–∞–ø—Ä–æ—Å —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω –∫–∞–∫ –ø—Ä–æ—Å—Ç–æ–π. –ù–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä–∞–∑–±–∏–µ–Ω–∏–µ.")
        return False, [user_message]

    messages = [
        {"role": "system", "content": decomposition_system_content},
        {"role": "user",
         "content": f"–†–∞–∑–±–µ–π –¥–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –Ω–∞ –ø–æ–¥–∑–∞–ø—Ä–æ—Å—ã: '{user_message}'. –û—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∫—Ä–∞—Ç–∫–∏–º–∏ –∏ –Ω–∞ —Ç–æ–º –∂–µ —è–∑—ã–∫–µ."}
    ]

    try:
        response = await openai_post_request(
            messages, decomposition_model_name, decomposition_max_tokens, decomposition_temperature
        )
        assistant_reply = response['choices'][0]['message']['content'].strip()

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å–ø–∏—Å–æ–∫ –ø–æ–¥–∑–∞–ø—Ä–æ—Å–æ–≤
        subqueries = assistant_reply.split('\n')
        subqueries = [q.strip() for q in subqueries if q.strip()]

        # –û—á–∏—Å—Ç–∫–∞ –ø–æ–¥–∑–∞–ø—Ä–æ—Å–æ–≤ –æ—Ç –Ω–æ–º–µ—Ä–æ–≤
        cleaned_subqueries = []
        for q in subqueries:
            q = re.sub(r'^\d+\.\s*', '', q)  # –£–¥–∞–ª—è–µ–º –Ω–æ–º–µ—Ä –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫–∏
            cleaned_subqueries.append(q)

        logging.info("üîç –†–∞–∑–±–∏–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –ø–æ–¥–∑–∞–ø—Ä–æ—Å—ã:")
        for subquery in cleaned_subqueries:
            logging.info(f"  üî∏ {subquery}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ–¥–∑–∞–ø—Ä–æ—Å–æ–≤
        if len(cleaned_subqueries) <= 1:
            logging.info("‚úÖ –ó–∞–ø—Ä–æ—Å –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —Ä–∞–∑–±–∏–µ–Ω–∏—è.")
            return False, [user_message]

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å—Ö–æ–∂–µ—Å—Ç—å –ø–æ–¥–∑–∞–ø—Ä–æ—Å–æ–≤ —Å –∏—Å—Ö–æ–¥–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º
        similarity_scores = [util.cos_sim(semantic_model.encode(user_message), semantic_model.encode(q))[0][0].item() for q in cleaned_subqueries]
        avg_similarity = sum(similarity_scores) / len(similarity_scores)

        if avg_similarity > 0.9:
            logging.info("‚úÖ –ü–æ–¥–∑–∞–ø—Ä–æ—Å—ã —Å–ª–∏—à–∫–æ–º –ø–æ—Ö–æ–∂–∏ –Ω–∞ –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å. –ù–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä–∞–∑–±–∏–µ–Ω–∏–µ.")
            return False, [user_message]

        is_complex = True
        return is_complex, cleaned_subqueries
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∏ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏: {e}")
        return False, [f"Error: {str(e)}"]

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–¥–∑–∞–ø—Ä–æ—Å–∞
async def process_subquery(subquery):
    logging.info(f"‚û°Ô∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–¥–∑–∞–ø—Ä–æ—Å–∞: {subquery}")

    # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    recommended_model = recommend_llm_by_semantics(subquery)

    messages = [
        {"role": "system", "content": default_system_content},
        {"role": "user", "content": subquery}
    ]

    try:
        response = await openai_post_request(
            messages, recommended_model, decomposition_max_tokens, decomposition_temperature
        )
        assistant_reply = response['choices'][0]['message']['content'].strip()
        logging.info(f"‚úîÔ∏è –û—Ç–≤–µ—Ç –Ω–∞ –ø–æ–¥–∑–∞–ø—Ä–æ—Å: {assistant_reply}")
        return assistant_reply
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–¥–∑–∞–ø—Ä–æ—Å–∞: {e}")
        return f"Error: {str(e)}"

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤
async def aggregate_responses_with_llm(subquery_responses):
    logging.info("üîÑ –°–ª–∏—è–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ–¥–∑–∞–ø—Ä–æ—Å–æ–≤:")

    aggregation_prompt = "–í–æ—Ç –æ—Ç–≤–µ—Ç—ã –Ω–∞ –ø–æ–¥–∑–∞–ø—Ä–æ—Å—ã:\n"
    for i, response in enumerate(subquery_responses, 1):
        logging.info(f"  {i}. {response}")
        aggregation_prompt += f"–ü–æ–¥–∑–∞–ø—Ä–æ—Å {i}: {response}\n"

    aggregation_prompt += "–°–æ–±–µ—Ä–∏ —ç—Ç–∏ –æ—Ç–≤–µ—Ç—ã –≤ –∫–æ—Ä–æ—Ç–∫–∏–π, —Å–≤—è–∑–Ω—ã–π –∏ —á–µ—Ç–∫–∏–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ —Ç–æ–º –∂–µ —è–∑—ã–∫–µ, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –±—ã–ª –∏—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å."

    messages = [
        {"role": "system", "content": aggregation_system_content},
        {"role": "user", "content": aggregation_prompt}
    ]

    try:
        response = await openai_post_request(
            messages, aggregation_model_name, aggregation_max_tokens, aggregation_temperature
        )
        aggregated_response = response['choices'][0]['message']['content'].strip()

        logging.info(f"üéØ –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: {aggregated_response}")
        return aggregated_response
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏: {e}")
        return f"Error: {str(e)}"

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–∞ –≤ OpenAI API
async def openai_post_request(messages, model_name, max_tokens, temperature):
    url = "https://api.openai.com/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",  # API –∫–ª—é—á, –ø–æ–ª—É—á–µ–Ω–Ω—ã–π –∏–∑ config.ini
        "Content-Type": "application/json",
    }
    payload = {
        "model": model_name,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature
    }

    retries = 3  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
    for attempt in range(retries):
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(url, json=payload, headers=headers)
            response.raise_for_status()  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞
            return response.json()  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º JSON-–æ—Ç–≤–µ—Ç –æ—Ç OpenAI API
        except httpx.ReadTimeout:
            logging.warning(f"‚è≥ –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–æ–ø—ã—Ç–∫–µ {attempt + 1} –æ–±—Ä–∞—â–µ–Ω–∏—è –∫ OpenAI API.")
            if attempt == retries - 1:
                raise
        except httpx.HTTPStatusError as e:
            logging.error(f"‚ùå HTTP –æ—à–∏–±–∫–∞: {e.response.status_code} - {e.response.text}")
            raise

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –º–æ–¥–µ–ª–µ–π
def load_model_recommendations():
    config = configparser.ConfigParser()
    # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫–∏ UTF-8
    with open('models_config.ini', 'r', encoding='utf-8') as f:
        config.read_file(f)

    model_recommendations = {}
    available_topics = []

    for topic, model in config['models'].items():
        available_topics.append(topic)
        model_recommendations[topic] = model

    return available_topics, model_recommendations

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
def recommend_llm_by_semantics(user_message):
    available_topics, model_recommendations = load_model_recommendations()

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ç–µ–º
    topic_embeddings = semantic_model.encode(available_topics, convert_to_tensor=True)

    # –≠–º–±–µ–¥–¥–∏–Ω–≥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
    message_embedding = semantic_model.encode(user_message, convert_to_tensor=True)

    # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
    similarities = util.cos_sim(message_embedding, topic_embeddings)[0]

    # –ù–∞—Ö–æ–¥–∏–º —Ç–µ–º—É —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º —Å—Ö–æ–¥—Å—Ç–≤–æ–º
    best_match_idx = similarities.argmax()
    best_match_topic = available_topics[best_match_idx]
    best_match_score = similarities[best_match_idx].item()

    # –í—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –Ω–∞–π–¥–µ–Ω–Ω–æ–π —Ç–µ–º–µ
    selected_model = model_recommendations.get(best_match_topic, model_recommendations.get('Default', 'gpt-3.5-turbo'))

    # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    logging.info(f"ü§ñ –í—ã–±—Ä–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ–º—ã '{best_match_topic}' (—Å—Ö–æ–¥—Å—Ç–≤–æ {best_match_score:.2f}): {selected_model}")

    return selected_model

if __name__ == '__main__':
    logging.info("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–µ—Ä–∞ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")

    # –ß—Ç–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    config = configparser.ConfigParser()
    with open('config.ini', 'r', encoding='utf-8') as configfile:
        config.read_file(configfile)

    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ OpenAI API –∫–ª—é—á–∞
    api_key = config['openai']['api_key']

    # –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –º–æ–¥–µ–ª–µ–π
    decomposition_model_name = config['decomposition_model']['name']
    decomposition_max_tokens = config['decomposition_model'].getint('max_tokens')
    decomposition_temperature = config['decomposition_model'].getfloat('temperature')

    aggregation_model_name = config['aggregation_model']['name']
    aggregation_max_tokens = config['aggregation_model'].getint('max_tokens')
    aggregation_temperature = config['aggregation_model'].getfloat('temperature')

    # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤
    default_system_content = config['prompt']['default_system_content']
    decomposition_system_content = config['prompt']['decomposition_system_content']
    aggregation_system_content = config['prompt']['aggregation_system_content']

    # –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
    similarity_threshold = config['semantic_search'].getfloat('similarity_threshold', fallback=0.5)
    embedding_model_name = config['semantic_search']['embedding_model_name']

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏
    logging.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏ '{embedding_model_name}'...")
    semantic_model = SentenceTransformer(embedding_model_name)
    logging.info(f"‚úÖ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å '{embedding_model_name}' –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ñ–ª–∞–≥–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–µ—Ä–∞
    server_ready = True
    logging.info("üöÄ –°–µ—Ä–≤–µ—Ä –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ.")

    app.run(debug=False)
